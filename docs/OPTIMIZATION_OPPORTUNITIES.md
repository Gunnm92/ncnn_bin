# Opportunit√©s d'Optimisation Performance

**Date:** 2026-01-30
**Contexte:** Batching n'am√©liore pas la performance (GPU satur√©)
**Baseline actuel:** 0.83 imgs/sec (images moyennes), 5.98 imgs/sec (petites)

---

## üéØ Analyse du Goulot d'√âtranglement Actuel

### Performance Mesur√©e (GPU mode)

| Taille Image | Temps GPU | Temps Total | GPU Utilization |
|--------------|-----------|-------------|-----------------|
| < 300 KB | 150-200 ms | 170 ms | ~90% |
| 500 KB | 2000 ms | 2100 ms | ~95% |
| 1-2 MB | 2200-2500 ms | 2400 ms | ~95% |

**Observation:** GPU proche de 100% ‚Üí Pas de marge pour parall√©lisme

---

## üöÄ Options d'Optimisation Disponibles

### Option 1: Pipeline CPU/GPU (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Recommand√©)

**Principe:** D√©coupler les √©tapes CPU et GPU

```
Actuellement (s√©quentiel):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Decode  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ   GPU   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ Encode  ‚îÇ  Image 1
‚îÇ  JPEG   ‚îÇ     ‚îÇ Upscale ‚îÇ     ‚îÇ  WebP   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   50ms           2000ms           50ms
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2100ms total ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Optimis√© (pipeline):
Thread 1: Decode  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ
Thread 2: GPU     ‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ
Thread 3: Encode  ‚îÇ    ‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>
          Throughput: ~1.2x faster (overlap CPU/GPU)
```

**Gains estim√©s:**
- ‚úÖ +20-30% throughput
- ‚úÖ Meilleure utilisation CPU et GPU
- ‚úÖ Pas de changement protocole

**Impl√©mentation:**
```cpp
// stdin_mode.cpp: run_keep_alive_protocol_v2()

// Queue 1: Images d√©cod√©es pr√™tes pour GPU
BlockingQueue<DecodedImage> decode_queue;

// Queue 2: Images upscal√©es pr√™tes pour encode
BlockingQueue<UpscaledImage> encode_queue;

// Thread 1: Decode (CPU)
std::thread decode_thread([&]() {
    while (auto img = read_next_jpeg()) {
        auto decoded = decode_jpeg(img);  // stb_image
        decode_queue.push(decoded);
    }
});

// Thread 2: Upscale (GPU) - thread principal
while (auto decoded = decode_queue.pop()) {
    auto upscaled = gpu_upscale(decoded);
    encode_queue.push(upscaled);
}

// Thread 3: Encode (CPU)
std::thread encode_thread([&]() {
    while (auto upscaled = encode_queue.pop()) {
        auto webp = encode_webp(upscaled);  // libwebp
        write_response(webp);
    }
});
```

**Effort:** Moyen (2-3 jours)

---

### Option 2: R√©duction Tile Size (‚≠ê‚≠ê‚≠ê Efficace petites machines)

**Principe:** Tiles plus petits = moins de RAM mais plus de calcul

```bash
# Actuel
--tile-size 512  # 512x512 tiles

# Petites machines
--tile-size 256  # 256x256 tiles
```

**Impact:**

| Tile Size | RAM GPU | Temps Total | Use Case |
|-----------|---------|-------------|----------|
| 0 (auto) | 1.5 GB | 2.0s | GPU puissant |
| 512 | 800 MB | 2.4s | Standard ‚úÖ |
| 256 | 400 MB | 2.8s | RAM limit√©e |
| 128 | 200 MB | 3.5s | Tr√®s petite GPU |

**Gains:**
- ‚úÖ -50% RAM avec tile-size 256
- ‚ö†Ô∏è -15% performance

**D√©j√† disponible:** Oui (via CLI)

---

### Option 3: Model Quantization INT8 (‚≠ê‚≠ê‚≠ê‚≠ê Recommand√©)

**Principe:** Mod√®le FP16/INT8 au lieu de FP32

```
FP32 (actuel):
  - Pr√©cision: 32 bits
  - Taille mod√®le: 100%
  - Vitesse: baseline
  - Qualit√©: 100%

FP16 (half precision):
  - Pr√©cision: 16 bits
  - Taille mod√®le: 50%
  - Vitesse: 1.5-2x faster (GPUs modernes)
  - Qualit√©: 99.5%

INT8 (quantized):
  - Pr√©cision: 8 bits
  - Taille mod√®le: 25%
  - Vitesse: 2-4x faster
  - Qualit√©: 98% (acceptable pour upscale)
```

**Gains estim√©s:**
- ‚úÖ +50-100% throughput
- ‚úÖ -50% RAM
- ‚ö†Ô∏è -1-2% qualit√© (imperceptible)

**Impl√©mentation:**
N√©cessite reconversion du mod√®le avec NCNN tools:
```bash
# Convertir mod√®le existant en INT8
ncnnoptimize model.param model.bin \
  model-int8.param model-int8.bin \
  65536
```

**Effort:** Faible (1 jour + tests qualit√©)

---

### Option 4: Vulkan Compute Shaders (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Maximum perf)

**Principe:** Utiliser Vulkan au lieu de CUDA/OpenCL

NCNN supporte d√©j√† Vulkan, mais n√©cessite compilation avec flag:

```cmake
# CMakeLists.txt
option(NCNN_VULKAN "vulkan compute shader support" ON)
```

**Gains (vs CUDA sur m√™me GPU):**
- ‚úÖ +10-30% throughput (selon GPU)
- ‚úÖ Meilleure compatibilit√© multi-GPU
- ‚úÖ Moins de driver overhead

**Effort:** Faible (recompilation)

---

### Option 5: Multi-GPU Parallelism (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Si plusieurs GPUs)

**Principe:** Distribuer requ√™tes sur plusieurs GPUs

```
Actuel (1 GPU):
  GPU 0: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Optimis√© (2 GPUs):
  GPU 0: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  GPU 1:         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> 2x throughput
```

**Impl√©mentation backend (Rust):**
```rust
// Pool de workers, un par GPU
let workers = vec![
    spawn_worker("--gpu-id 0"),
    spawn_worker("--gpu-id 1"),
];

// Round-robin distribution
for (idx, request) in requests.enumerate() {
    let worker = &workers[idx % workers.len()];
    worker.send(request);
}
```

**Gains:**
- ‚úÖ Throughput √ó nombre de GPUs (lin√©aire)
- ‚úÖ Aucune modification binaire C++
- ‚úÖ Scaling horizontal

**Effort:** Moyen (backend Rust)

---

### Option 6: Pr√©chargement Mod√®le en VRAM (‚≠ê‚≠ê Marginal)

**Principe:** Garder mod√®le en VRAM entre requ√™tes

**Actuellement:** D√©j√† impl√©ment√© ! (keep-alive)
- ‚ùå Pas de gain suppl√©mentaire

---

### Option 7: Compression Input/Output Adaptative (‚≠ê‚≠ê‚≠ê Bandwidth)

**Principe:** Ajuster qualit√© selon taille image

```cpp
// Pour petites images: qualit√© maximale
if (image_size < 500_KB) {
    webp_quality = 95;
}
// Pour grandes images: qualit√© r√©duite
else if (image_size > 2_MB) {
    webp_quality = 85;  // -10 quality
}
```

**Gains:**
- ‚úÖ -20-30% taille output grandes images
- ‚úÖ -10-15% temps encode
- ‚ö†Ô∏è L√©g√®re perte qualit√©

**Effort:** Faible (1h)

---

### Option 8: Async I/O avec io_uring (‚≠ê‚≠ê‚≠ê‚≠ê Linux only)

**Principe:** Overlap disk I/O avec GPU compute

Pour batch file processing (pas stdin mode):
```cpp
// Lire fichiers de mani√®re async pendant GPU compute
io_uring_queue_init(32, &ring, 0);

// Queue read operations
for (auto& file : files) {
    io_uring_prep_read(...);
}

// Process pendant que I/O en cours
while (pending_io || pending_gpu) {
    // ...
}
```

**Gains:**
- ‚úÖ +15-25% throughput (file mode)
- ‚ùå Pas applicable stdin mode

**Effort:** √âlev√© (3-4 jours)

---

## üìä Comparaison des Options

| Option | Gain Throughput | Effort | Priorit√© | Applicable |
|--------|----------------|--------|----------|------------|
| **Pipeline CPU/GPU** | +20-30% | Moyen | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Tous modes |
| **Model INT8** | +50-100% | Faible | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Si qualit√© OK |
| **Vulkan** | +10-30% | Faible | ‚≠ê‚≠ê‚≠ê‚≠ê | Tous |
| **Multi-GPU** | √óN GPUs | Moyen | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Si >1 GPU |
| **Tile size** | -15% mais -50% RAM | Aucun | ‚≠ê‚≠ê‚≠ê | D√©j√† dispo |
| **Compression adaptive** | +10-15% | Faible | ‚≠ê‚≠ê‚≠ê | Output |
| **Async I/O** | +15-25% | √âlev√© | ‚≠ê‚≠ê | File mode |

---

## üéØ Recommandations par Sc√©nario

### Sc√©nario 1: Gain Rapide (1-2 jours)

**Actions:**
1. ‚úÖ **Activer Vulkan** (recompilation)
2. ‚úÖ **Model INT8** (reconversion + test qualit√©)

**Gain attendu:** +60-130% throughput

### Sc√©nario 2: Optimisation Pouss√©e (1 semaine)

**Actions:**
1. ‚úÖ Pipeline CPU/GPU (threading)
2. ‚úÖ Model INT8
3. ‚úÖ Vulkan

**Gain attendu:** +100-200% throughput

### Sc√©nario 3: Multi-GPU (si mat√©riel disponible)

**Actions:**
1. ‚úÖ Multi-GPU backend (Rust)
2. ‚úÖ Pipeline CPU/GPU
3. ‚úÖ Model INT8

**Gain attendu:** +200-400% throughput (2-4 GPUs)

---

## üî¨ Tests de Validation N√©cessaires

### Pour INT8 Quantization

```python
# Comparer qualit√© FP32 vs INT8
original_output = process_with_fp32(image)
int8_output = process_with_int8(image)

# M√©triques
psnr = calculate_psnr(original_output, int8_output)
ssim = calculate_ssim(original_output, int8_output)

# Acceptable si:
# PSNR > 35 dB
# SSIM > 0.95
```

### Pour Pipeline CPU/GPU

```python
# V√©rifier pas de r√©gression
baseline_time = measure_sequential()
pipeline_time = measure_pipeline()

speedup = baseline_time / pipeline_time
assert speedup > 1.15  # Au moins +15%
```

---

## üí° Quick Wins Imm√©diats

### 1. Vulkan (10 minutes)

```bash
cd bdreader-ncnn-upscaler/build-release
cmake .. -DNCNN_VULKAN=ON
make -j4

# Test
./bdreader-ncnn-upscaler --mode stdin --keep-alive --gpu-id 0
```

### 2. Tile-size tuning (0 minutes, d√©j√† dispo)

```bash
# Pour RAM limit√©e
--tile-size 256

# Pour performance max
--tile-size 0  # Auto-detect optimal
```

### 3. Multi-GPU backend (si 2+ GPUs)

```bash
# Lancer 2 workers en parall√®le
./bdreader-ncnn-upscaler --gpu-id 0 --keep-alive &
./bdreader-ncnn-upscaler --gpu-id 1 --keep-alive &

# Load balancer Rust distribue requ√™tes
```

---

## üéì Conclusion

**Meilleur ROI (Return on Investment):**

1. **Multi-GPU** (si disponible): √ó2-4 throughput, effort moyen
2. **Model INT8**: +50-100%, effort faible, risque qualit√©
3. **Vulkan**: +10-30%, effort minimal
4. **Pipeline CPU/GPU**: +20-30%, effort moyen

**Pour d√©marrer:**
1. Tester Vulkan (10 min)
2. Si >1 GPU: setup multi-GPU backend
3. Tester INT8 quantization avec validation qualit√©
4. Si besoin plus: impl√©menter pipeline

**Gain total potentiel: +300-500% throughput** üöÄ

---

**Prochaines √©tapes sugg√©r√©es:**
1. Benchmark Vulkan vs CUDA/OpenCL
2. Convertir mod√®le en INT8 + tests qualit√©
3. POC pipeline CPU/GPU sur 1 image
